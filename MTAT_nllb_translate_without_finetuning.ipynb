{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 10931104,
          "sourceType": "datasetVersion",
          "datasetId": 6796905
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "MTAT -- nllb_translate_without_finetuning",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiterharris/Assignment-1/blob/master/MTAT_nllb_translate_without_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Install Dependencies\n"
      ],
      "metadata": {
        "id": "pwBjwcuFlzLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers sentencepiece torch\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-01T17:41:41.031514Z",
          "iopub.execute_input": "2025-06-01T17:41:41.031811Z",
          "iopub.status.idle": "2025-06-01T17:41:44.434282Z",
          "shell.execute_reply.started": "2025-06-01T17:41:41.031789Z",
          "shell.execute_reply": "2025-06-01T17:41:44.433222Z"
        },
        "id": "GnOAP8o2lzLk",
        "outputId": "a8097c4f-3110-4a99-b8a4-8a1d493bba50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Pretrained NLLB Model\n",
        "\n",
        "There are multiple NLLB model sizes, but for Kaggle (to avoid memory issues), use the distilled version:"
      ],
      "metadata": {
        "id": "g029a4C7lzLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Change to larger models if needed\n",
        "\n",
        "# Load tokenizer & model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cuda\")  # Move to GPU\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-01T17:41:44.435598Z",
          "iopub.execute_input": "2025-06-01T17:41:44.435857Z",
          "iopub.status.idle": "2025-06-01T17:41:47.332136Z",
          "shell.execute_reply.started": "2025-06-01T17:41:44.435813Z",
          "shell.execute_reply": "2025-06-01T17:41:47.331154Z"
        },
        "id": "udAh0XsilzLk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Source & Target Language Tokens\n",
        "\n",
        "Each language in NLLB-200 has a special token (<xxx_Latn>). Find the correct tokens from NLLB-200 Language Codes."
      ],
      "metadata": {
        "id": "-Lraut2vlzLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_LANG = \"<deu_Latn>\"  # Source: German\n",
        "TGT_LANG = \"<eng_Latn>\"  # Target: English"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-01T17:41:47.335496Z",
          "iopub.execute_input": "2025-06-01T17:41:47.335763Z",
          "iopub.status.idle": "2025-06-01T17:41:47.340126Z",
          "shell.execute_reply.started": "2025-06-01T17:41:47.335741Z",
          "shell.execute_reply": "2025-06-01T17:41:47.339266Z"
        },
        "id": "MW8RKYA_lzLl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Translation Function\n",
        "\n",
        "Now, create a function that translates a batch of sentences."
      ],
      "metadata": {
        "id": "Bs4mXVGGlzLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def translate(sentences, model, tokenizer, src_lang, tgt_lang, max_length=128, batch_size=4):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "\n",
        "    translations = []\n",
        "\n",
        "    for i in range(0, len(sentences), batch_size):\n",
        "        batch = sentences[i:i + batch_size]\n",
        "        batch = [src_lang + sentence for sentence in batch]  # Add language token\n",
        "\n",
        "        # Tokenize inputs\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
        "\n",
        "        # Generate translations\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_length=max_length, forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang))\n",
        "\n",
        "        # Decode and store translations\n",
        "        translations.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
        "\n",
        "    return translations\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-01T17:41:47.341234Z",
          "iopub.execute_input": "2025-06-01T17:41:47.341507Z",
          "iopub.status.idle": "2025-06-01T17:41:47.353738Z",
          "shell.execute_reply.started": "2025-06-01T17:41:47.341482Z",
          "shell.execute_reply": "2025-06-01T17:41:47.352847Z"
        },
        "id": "N-NG8VKtlzLm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translate Example Sentences"
      ],
      "metadata": {
        "id": "ZUlant6olzLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = '/kaggle/input/mtat25-ted-data-test/data/TED2020.de-en.de.test'\n",
        "\n",
        "# Read test sentences from file\n",
        "with open(test_sentences, \"r\", encoding=\"utf-8\") as f:\n",
        "    test_sentences = [line.strip() for line in f.readlines() if line.strip()]  # Remove empty lines\n",
        "\n",
        "translated_sentences = translate(test_sentences, model, tokenizer, SRC_LANG, TGT_LANG)\n",
        "\n",
        "# Print translations\n",
        "#for src, tgt in zip(test_sentences, translated_sentences):\n",
        "#    print(f\"🔹 Source: {src}\\n🔹 Translation: {tgt}\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-01T17:41:47.35487Z",
          "iopub.execute_input": "2025-06-01T17:41:47.355189Z",
          "iopub.status.idle": "2025-06-01T17:41:47.408649Z",
          "shell.execute_reply.started": "2025-06-01T17:41:47.355163Z",
          "shell.execute_reply": "2025-06-01T17:41:47.407379Z"
        },
        "id": "tJ8FyNc5lzLm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Translations to a New File\n",
        "\n",
        "If you want to save the translated output, write it to a new file:"
      ],
      "metadata": {
        "id": "s1fvCJR7lzLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_file=\"TED2020.de-en.de.test.nllb2en\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for sentence in translated_sentences:\n",
        "        f.write(sentence + \"\\n\")\n",
        "\n",
        "print(f'Translations saved to {output_file}')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-01T17:41:47.40918Z",
          "iopub.status.idle": "2025-06-01T17:41:47.409417Z",
          "shell.execute_reply": "2025-06-01T17:41:47.409319Z"
        },
        "id": "m6yUgSgMlzLn"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}